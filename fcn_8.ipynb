{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1132317,"sourceType":"datasetVersion","datasetId":635428}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# make a vgg-fcn-8s , use vgg model and lets just start it\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom abc import ABCMeta\nimport torchvision.models as models\nimport torch\nfrom tqdm import tqdm\nimport os\nimport pandas as pd\nfrom torchvision.io import read_image\nimport torch\nimport torchvision.transforms.v2 as transforms\n!pip install torchsummary\nfrom torchsummary import summary\nimport random\nimport pickle\nfrom torch.optim.lr_scheduler import ExponentialLR\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\n#load in the data\nimport os\nimport pandas as pd\nimport numpy as np\nfrom torchvision.io import read_image\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import InterpolationMode","metadata":{"execution":{"iopub.status.busy":"2024-03-03T06:54:06.210581Z","iopub.execute_input":"2024-03-03T06:54:06.211069Z","iopub.status.idle":"2024-03-03T06:54:18.921259Z","shell.execute_reply.started":"2024-03-03T06:54:06.211038Z","shell.execute_reply":"2024-03-03T06:54:18.920093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg = models.vgg16(pretrained = True)\n# \"D\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"],\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nvgg.to(device)\nimport os\nimport pandas as pd\nfrom torchvision.io import read_image\nimport torch\nh = 720\nw = 960","metadata":{"execution":{"iopub.status.busy":"2024-03-03T06:54:18.923483Z","iopub.execute_input":"2024-03-03T06:54:18.924176Z","iopub.status.idle":"2024-03-03T06:54:20.798188Z","shell.execute_reply.started":"2024-03-03T06:54:18.924145Z","shell.execute_reply":"2024-03-03T06:54:20.797117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_hot_from_label(class_dict ,segmented_img):\n    semantic_map = []\n    \n    for color in class_dict.values():\n        \n        #color is a tensor of dim (3)\n        color = color.unsqueeze(dim = 1).unsqueeze(dim = 2)\n        equal = torch.eq(color , segmented_img)\n        semantic_map.append(torch.all(equal , axis = 0))\n        \n    return torch.stack(semantic_map)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T06:54:20.799483Z","iopub.execute_input":"2024-03-03T06:54:20.799813Z","iopub.status.idle":"2024-03-03T06:54:20.805686Z","shell.execute_reply.started":"2024-03-03T06:54:20.799789Z","shell.execute_reply":"2024-03-03T06:54:20.804760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rev_one_hot(class_dict , one_hot):\n    #print(\" 0000000000 \" + str(torch.sum(one_hot)) )\n    # one hot is of type 1s and 0s\n    i = 0\n    channels , height , width = one_hot.size()\n    rgb_image = torch.zeros(3 , height , width).to(device)\n    for color in class_dict.values():\n        \n        \n        rgb_image += torch.mul(one_hot[i].to(device) , color.unsqueeze(dim = 1).unsqueeze(dim = 2).to(device))\n        i+= 1\n        \n        \n    return rgb_image        \n                               \n                        ","metadata":{"execution":{"iopub.status.busy":"2024-03-03T06:54:20.808359Z","iopub.execute_input":"2024-03-03T06:54:20.808669Z","iopub.status.idle":"2024-03-03T06:54:20.820129Z","shell.execute_reply.started":"2024-03-03T06:54:20.808645Z","shell.execute_reply":"2024-03-03T06:54:20.819346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prob_to_one_hot(t):\n    \n    channels , height , width = t.size()\n    maximums = torch.argmax(t , dim = 0)\n    #maximums is of shape height*width\n    one_hot = torch.zeros((channels , height , width)).to(device)\n    one_hot[maximums , torch.arange(height).unsqueeze(1) , torch.arange(width).unsqueeze(0)] = 1\n    \n    return one_hot","metadata":{"execution":{"iopub.status.busy":"2024-03-03T06:54:20.821318Z","iopub.execute_input":"2024-03-03T06:54:20.821617Z","iopub.status.idle":"2024-03-03T06:54:20.833828Z","shell.execute_reply.started":"2024-03-03T06:54:20.821574Z","shell.execute_reply":"2024-03-03T06:54:20.833043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def intersection_over_union(one_hot_truth , one_hot_predicted):\n    sum = 0\n    classes = len(one_hot_truth)\n    \n    for i in range(len(one_hot_truth)):\n        \n        intersection = one_hot_truth[i]*one_hot_predicted[i]\n        intersection = torch.sum(intersection).item()\n        union = torch.sum(one_hot_truth[i]).item() + torch.sum(one_hot_predicted[i]).item() - intersection\n        \n        if(union == 0):\n            classes -= 1 # i think that if a class is not there then it should not affect the avg IOU\n            continue\n        \n        sum += intersection/union\n        \n    return (sum/len(one_hot_truth) , sum/classes) \n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-03T06:54:20.835065Z","iopub.execute_input":"2024-03-03T06:54:20.835813Z","iopub.status.idle":"2024-03-03T06:54:20.853291Z","shell.execute_reply.started":"2024-03-03T06:54:20.835780Z","shell.execute_reply":"2024-03-03T06:54:20.852483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pixel_accuracy(label, pred):\n    sum = 0\n\n    \n    channel , width , height = pred.size()\n    eq = torch.eq(label , pred)\n    sum+= torch.sum(torch.all(eq, dim = 0)).item()/(width*height)\n    \n    \n    return (sum)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-03T06:54:20.854460Z","iopub.execute_input":"2024-03-03T06:54:20.854753Z","iopub.status.idle":"2024-03-03T06:54:20.863330Z","shell.execute_reply.started":"2024-03-03T06:54:20.854729Z","shell.execute_reply":"2024-03-03T06:54:20.862636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mean_accuracy(one_hot_truth , one_hot_label):\n    sum = 0\n    channels , width , height = one_hot_truth.size()\n    for i in range (len(one_hot_truth)):\n        \n        \n        \n        true = torch.sum(torch.eq(one_hot_truth[i], 1) & torch.eq(one_hot_label[i], 1)).item()\n        \n        if(torch.sum(one_hot_truth[i]).item() == 0):\n            sum+= 1\n        else:\n            sum += (true/(torch.sum(one_hot_truth[i]).item()))\n       \n    \n    return (sum/channels)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-03T06:54:20.864463Z","iopub.execute_input":"2024-03-03T06:54:20.864840Z","iopub.status.idle":"2024-03-03T06:54:20.874200Z","shell.execute_reply.started":"2024-03-03T06:54:20.864816Z","shell.execute_reply":"2024-03-03T06:54:20.873460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dict_from_csv(file_path):\n    df = pd.read_csv(file_path)\n    mappings ={}\n    for i in range (len(df)):\n        t = torch.tensor((df.iloc[i,1] , df.iloc[i,2] , df.iloc[i,3]))\n        mappings[df.iloc[i,0]] = t\n    return mappings\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-03T06:54:20.875186Z","iopub.execute_input":"2024-03-03T06:54:20.875430Z","iopub.status.idle":"2024-03-03T06:54:20.885097Z","shell.execute_reply.started":"2024-03-03T06:54:20.875409Z","shell.execute_reply":"2024-03-03T06:54:20.884280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass vgg_fcn_32(nn.Module):\n  def __init__(self, vggnet, n_classes ):\n    super().__init__()\n    self.block1 = vggnet.features\n    \n    self.block3 = nn.Sequential(\n        nn.Conv2d(512 , 1024 , 1 , 1 ),\n        nn.Dropout(),\n        nn.Conv2d(1024 , 2048 , 1 , 1 ),\n        nn.Dropout(),\n        nn.Conv2d(2048 , 2048, 1 , 1 ),\n    )\n    #so in VGG , the final output is 1/32 ( each spatial dimension) , this reduction is done primarily by the MAX - pool layers\n    #now upscale\n    self.block4 = nn.Sequential(\n        nn.ConvTranspose2d(2048 , 2048 , 3 , 2 , padding = 1, output_padding = 1),\n        nn.ReLU(),\n        #nn.BatchNorm2d(2048),\n        nn.ConvTranspose2d(2048 , 1024 , 3 , 2 , padding = 1, output_padding = 1),\n        nn.ReLU(),\n        nn.BatchNorm2d(1024),\n        nn.ConvTranspose2d(1024 , 1024, 3 , 2 , padding = 1, output_padding = 1),\n        nn.ReLU(),\n        #nn.BatchNorm2d(1024),\n        nn.ConvTranspose2d(1024, 512 , 3 , 2 , padding = 1, output_padding = 1),\n        nn.ReLU(),\n        nn.BatchNorm2d(512),\n        nn.ConvTranspose2d(512 , n_classes , 3 , 2 , padding = 1, output_padding = 1),\n        nn.ReLU()\n    )\n    self.classifier = nn.Conv2d(n_classes , n_classes , 1 )\n    self.last = nn.Softmax2d()\n  def forward(self , x):\n    x = self.block1(x)\n    \n    x = self.block3(x)\n    \n    x = self.block4(x)\n    \n    x = self.classifier(x)\n    x = self.last(x)\n    \n    return x\n\n\n\n\n\n    \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-03T06:54:20.889676Z","iopub.execute_input":"2024-03-03T06:54:20.889930Z","iopub.status.idle":"2024-03-03T06:54:20.901284Z","shell.execute_reply.started":"2024-03-03T06:54:20.889909Z","shell.execute_reply":"2024-03-03T06:54:20.900421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class vgg_fcn_8(nn.Module):\n    def __init__(self, vggnet , n_classes):\n        super().__init__()\n        #output of pool3 + 2*2* ouput of pool 4 + 2* 1x1 convolutions(output of pool 5)\n        self.pool3 = vggnet.features[:17] #256 channels output\n        self.pool4 = vggnet.features[17:24] # 512 channels output\n        self.pool5 = vggnet.features[24:] #512 channels output  \n        self.convolutions1x1 = nn.Sequential(\n        nn.Conv2d(512 , 1024 , 1 , 1 ),\n        nn.Dropout(),\n        nn.Conv2d(1024 , 2048 , 1 , 1 ),\n        nn.Dropout(),\n        nn.Conv2d(2048 , 2048 , 1 , 1 ),\n        )\n        self.upsample_conv_7 = nn.Sequential(\n        #self.upscale_bilinear_1 = \n        nn.ConvTranspose2d(2048 , 1024 ,3 , 2 , padding = 1 , output_padding = 1),\n        nn.ReLU(),\n        nn.BatchNorm2d(1024),\n        #self.upscale_bilinear_2 = \n        nn.ConvTranspose2d(1024 , 512, 3, 2 , padding = 1, output_padding = 1),\n        nn.ReLU(),\n        nn.BatchNorm2d(512),\n        #self.dec_layer_1 =\n        nn.ConvTranspose2d(512 , 256 , 3 , 1 , padding = 1)\n        \n        )\n        \n        self.upsample_pool_4 = nn.Sequential(\n        #self.upscale_bilinear_3 = \n        nn.ConvTranspose2d(512 , 512 , 3 , 2, padding = 1 , output_padding = 1),\n        nn.ReLU(),\n        nn.BatchNorm2d(512),\n        #self.dec_layer_2 = \n        nn.ConvTranspose2d(512 , 256 , 3 , 1 , padding = 1)\n        )\n        self.block1 = nn.Sequential(\n        nn.ConvTranspose2d(256 , 1024 , 3 , 2 , padding = 1, output_padding = 1),\n        nn.ReLU(),\n        nn.BatchNorm2d(1024),\n        nn.ConvTranspose2d(1024 , 512 , 3 , 2 , padding = 1, output_padding = 1),\n        nn.ReLU(),\n        nn.BatchNorm2d(512),\n        nn.ConvTranspose2d(512 , n_classes , 3 , 2 , padding = 1, output_padding = 1),\n        nn.ReLU(),            \n        nn.Conv2d(n_classes , n_classes , 1),\n        nn.Softmax2d()\n        )\n    def forward(self , x):\n        pool3_output = self.pool3(x)\n        pool4_output = self.pool4(pool3_output)\n        pool5_output = self.pool5(pool4_output)\n        output_conv_7 = self.convolutions1x1(pool5_output)\n        upsampled_conv_7 = self.upsample_conv_7(output_conv_7)\n        upsampled_pool_4 = self.upsample_pool_4(pool4_output)\n        x = torch.add(pool3_output , torch.add( upsampled_conv_7 ,upsampled_pool_4))\n        output = self.block1(x)\n        return output\n        \n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-03T06:54:20.902830Z","iopub.execute_input":"2024-03-03T06:54:20.903107Z","iopub.status.idle":"2024-03-03T06:54:20.918513Z","shell.execute_reply.started":"2024-03-03T06:54:20.903084Z","shell.execute_reply":"2024-03-03T06:54:20.917838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trans = transforms.Compose([transforms.ToDtype(torch.float32 , scale = True), \n                            transforms.Resize((320 , 480))])\ntrans_target = transforms.Compose([transforms.ToDtype(torch.float32) , transforms.Resize((320 , 480) , interpolation=InterpolationMode.NEAREST_EXACT)])","metadata":{"execution":{"iopub.status.busy":"2024-03-03T06:54:20.919663Z","iopub.execute_input":"2024-03-03T06:54:20.919938Z","iopub.status.idle":"2024-03-03T06:54:20.937858Z","shell.execute_reply.started":"2024-03-03T06:54:20.919915Z","shell.execute_reply":"2024-03-03T06:54:20.937129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n#custom dataset class\nclass LoadCamvid(Dataset):\n    def __init__(self , csv_file , label_dir , img_dir , transform = None , target_transform = None):\n        self.img_dir = img_dir\n        self.label_dir = label_dir\n        self.csv_file = csv_file\n        self.transform = transform\n        self.target_transform = target_transform\n        self.class_data = pd.read_csv(csv_file)\n        self.dict = {}\n        self.img_files = os.listdir(self.img_dir)\n        self.label_files = os.listdir(self.label_dir)\n        df = pd.read_csv(self.csv_file)\n        \n        self.img_files.sort()\n        self.label_files.sort()\n        \n        for i in range(len(df)):\n            \n            t = torch.tensor((df.iloc[i,1] , df.iloc[i,2] , df.iloc[i,3]))\n            self.dict[df.iloc[i,0]] = t\n    \n        \n        \n    def __len__(self):\n        return len(self.img_files)\n    \n    def __getitem__(self , idx):\n        # i will return onehot encoding\n        label = read_image(os.path.join(self.label_dir , self.label_files[idx]))\n        label.to(device)\n        \n        onehot = one_hot_from_label(self.dict , label)\n        onehot.to(device)\n        img = read_image(os.path.join(self.img_dir , self.img_files[idx]))\n        img.to(device)\n        img = self.transform(img)\n        onehot = self.target_transform(onehot)       \n        label = self.target_transform(label)\n        \n        return (img , onehot ,label)\n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-03T06:54:20.938969Z","iopub.execute_input":"2024-03-03T06:54:20.939251Z","iopub.status.idle":"2024-03-03T06:54:20.956161Z","shell.execute_reply.started":"2024-03-03T06:54:20.939229Z","shell.execute_reply":"2024-03-03T06:54:20.955344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data  = LoadCamvid(\"/kaggle/input/camvid/CamVid/class_dict.csv\" ,\"/kaggle/input/camvid/CamVid/train_labels\" ,  \"/kaggle/input/camvid/CamVid/train\" , transform = trans , target_transform = trans_target)\nvalidation_data= LoadCamvid(\"/kaggle/input/camvid/CamVid/class_dict.csv\" , \"/kaggle/input/camvid/CamVid/val_labels\" ,\"/kaggle/input/camvid/CamVid/val\" , transform = trans , target_transform = trans_target )\n\npartial_set = torch.utils.data.Subset(training_data , range(4))\n\ntrain_dataloader = DataLoader(training_data , batch_size =16 , shuffle = True)\nval_dataloader = DataLoader(validation_data , batch_size = 16 , shuffle = True)\n\ndf = pd.read_csv(\"/kaggle/input/camvid/CamVid/class_dict.csv\")\n\n\n\nmappings ={}\nfor i in range (len(df)):\n    t = torch.tensor((df.iloc[i,1] , df.iloc[i,2] , df.iloc[i,3]))\n    mappings[df.iloc[i,0]] = t","metadata":{"execution":{"iopub.status.busy":"2024-03-03T06:54:20.957309Z","iopub.execute_input":"2024-03-03T06:54:20.958049Z","iopub.status.idle":"2024-03-03T06:54:21.010543Z","shell.execute_reply.started":"2024-03-03T06:54:20.958006Z","shell.execute_reply":"2024-03-03T06:54:21.009792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model\nfcn = vgg_fcn_8(vgg , 32)\nfcn.to(device)\nsummary(fcn  , input_size = (3 , 320 , 480))\nfcn = nn.DataParallel(fcn)\nlayer_wise_gradient = {}\nfor name, param in fcn.named_parameters():\n    if('weight' in name):\n        \n        layer_wise_gradient[name] = []","metadata":{"execution":{"iopub.status.busy":"2024-03-03T06:54:21.011785Z","iopub.execute_input":"2024-03-03T06:54:21.012095Z","iopub.status.idle":"2024-03-03T06:54:21.860717Z","shell.execute_reply.started":"2024-03-03T06:54:21.012069Z","shell.execute_reply":"2024-03-03T06:54:21.859771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training loop\nepochs = 75\n\n\ntrain_loss = []\nepoch_count = []\nval_loss= []\n#train_pixelwise_accuracy= []\nval_pixelwise_accuracy = []\n#train_mean_accuracy = []\nval_mean_accuracy = []\n#train_iou = []\nval_iou = []\n\nloss_fnc = nn.BCELoss()\noptim = torch.optim.Adam(fcn.parameters() , lr = 0.0001)\nscheduler = ExponentialLR(optim, gamma=0.985) \nfor epoch in tqdm(range(epochs)):\n    train_loss.append(0)\n    epoch_count.append(epoch)\n    val_loss.append(0)\n    #train_pixelwise_accuracy.append(0)\n    val_pixelwise_accuracy.append(0)\n    #train_mean_accuracy.append(0)\n    val_mean_accuracy.append(0)\n    #train_iou.append(0)\n    val_iou.append(0)\n    clip_value = 1\n    \n    for val_batch_idx , (img , one_hot , label) in enumerate(val_dataloader):\n        fcn.eval()\n        with torch.no_grad():\n            \n            img = img.to(device)\n            one_hot = one_hot.to(device)\n            label = label.to(device)\n            \n            output = fcn(img)\n            \n            val_loss[epoch] += loss_fnc(output , one_hot).item()\n            \n            \n            for i in range(len(output)):\n                j  = random.randint(0 , len(output)-1)\n                \n                one_hot_output = prob_to_one_hot(output[j])\n                \n                output_label = rev_one_hot(mappings , one_hot_output)\n\n                \n                val_iou[epoch] += intersection_over_union(one_hot[j] , one_hot_output)[1]\n                val_pixelwise_accuracy[epoch] += pixel_accuracy(label[j] , output_label)\n                val_mean_accuracy[epoch] += mean_accuracy(one_hot[j] , one_hot_output)\n                \n    val_iou[epoch] /= 100\n    val_pixelwise_accuracy[epoch] /= 100\n    val_mean_accuracy[epoch] /= 100\n            \n    for batch_idx , (img , one_hot , label) in enumerate(train_dataloader):\n        fcn.train()\n        torch.cuda.empty_cache()\n        img = img.to(device)\n        one_hot = one_hot.to(device)\n        output = fcn(img)\n        loss_val = loss_fnc(output , one_hot)\n        \n        train_loss[epoch] += loss_val.item()\n        optim.zero_grad()\n        loss_val.backward()\n        if epoch%5 == 0:\n            for name , param in fcn.named_parameters():\n                if('weight' in name):\n\n                    layer_wise_gradient[name].append(param.grad.norm().item())\n                    \n        torch.nn.utils.clip_grad_norm_(fcn.parameters(), clip_value)\n        optim.step()\n        \n        \n        \n    \n    scheduler.step()     \n    train_loss[epoch] = train_loss[epoch]/369\n    val_loss[epoch] = val_loss[epoch]/100    \n    if epoch%5 == 0:\n        torch.save(fcn.state_dict(), \"fcn8_\" + str(epoch) +\".pth\")\n    print(f\"Learning Rate: {optim.param_groups[0]['lr']}\")\n    print(f\"epoch - : {epoch_count[epoch]} ,training loss - : {train_loss[epoch]} , validation los - : {val_loss[epoch]}\" )\n    \n    \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-03T06:54:21.861903Z","iopub.execute_input":"2024-03-03T06:54:21.862188Z","iopub.status.idle":"2024-03-03T07:05:17.439770Z","shell.execute_reply.started":"2024-03-03T06:54:21.862164Z","shell.execute_reply":"2024-03-03T07:05:17.438709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T07:05:17.440998Z","iopub.execute_input":"2024-03-03T07:05:17.441279Z","iopub.status.idle":"2024-03-03T07:05:17.616924Z","shell.execute_reply.started":"2024-03-03T07:05:17.441254Z","shell.execute_reply":"2024-03-03T07:05:17.615835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting the gradiesnts of all the layers (trivial)\nplt.figure(figsize=(12, 12))\nprint(type(layer_wise_gradient.items()))\nfor index,(key, values) in enumerate(layer_wise_gradient.items()):\n    #if('block4' in key):\n        plt.plot(values, label=key)\n\n# Adding labels and title\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Line Graph for Each Key')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T07:05:17.618628Z","iopub.execute_input":"2024-03-03T07:05:17.619079Z","iopub.status.idle":"2024-03-03T07:05:18.392548Z","shell.execute_reply.started":"2024-03-03T07:05:17.619050Z","shell.execute_reply":"2024-03-03T07:05:18.391642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting the losses\nplt.plot(epoch_count, train_loss , label = 'train_loss')\nplt.plot(epoch_count, val_loss , label = 'val_loss')\nplt.title('Training Loss over Epochs')\nplt.legend()\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T07:05:18.394036Z","iopub.execute_input":"2024-03-03T07:05:18.394712Z","iopub.status.idle":"2024-03-03T07:05:18.645872Z","shell.execute_reply.started":"2024-03-03T07:05:18.394676Z","shell.execute_reply":"2024-03-03T07:05:18.644878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting IOU over the validation dataset\nplt.plot(epoch_count, val_iou , label = 'val_iou')\nplt.title('IOU over Epochs')\nplt.legend()\nplt.xlabel('Epochs')\nplt.ylabel('IOU')\n\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T07:05:18.647203Z","iopub.execute_input":"2024-03-03T07:05:18.647664Z","iopub.status.idle":"2024-03-03T07:05:18.871362Z","shell.execute_reply.started":"2024-03-03T07:05:18.647630Z","shell.execute_reply":"2024-03-03T07:05:18.870514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting mean_accuracy over the validation dataset\nplt.plot(epoch_count, val_mean_accuracy , label = 'val_iou')\nplt.title('mean_accuracy over Epochs')\nplt.legend()\nplt.xlabel('Epochs')\nplt.ylabel('mean acc')\n\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T07:05:18.872329Z","iopub.execute_input":"2024-03-03T07:05:18.872607Z","iopub.status.idle":"2024-03-03T07:05:19.091538Z","shell.execute_reply.started":"2024-03-03T07:05:18.872567Z","shell.execute_reply":"2024-03-03T07:05:19.090631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting pixel accuracy over the validation dataset\nplt.plot(epoch_count, val_pixelwise_accuracy , label = 'val_pixelwise_accuracy')\nplt.title('pixelwise_accuracy over Epochs')\nplt.legend()\nplt.xlabel('Epochs')\nplt.ylabel('pixelwise acc')\n\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T07:05:19.092585Z","iopub.execute_input":"2024-03-03T07:05:19.092865Z","iopub.status.idle":"2024-03-03T07:05:19.342581Z","shell.execute_reply.started":"2024-03-03T07:05:19.092841Z","shell.execute_reply":"2024-03-03T07:05:19.341680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#saving the status\nwith open('record.pkl' , 'wb') as f:\n    pickle.dump(layer_wise_gradient , f)\n    pickle.dump(train_loss , f)\n    pickle.dump(val_loss , f)\n    pickle.dump(val_iou , f)\n    pickle.dump(val_mean_accuracy , f)\n    pickle.dump(val_pixelwise_accuracy , f)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T07:05:19.344009Z","iopub.execute_input":"2024-03-03T07:05:19.344799Z","iopub.status.idle":"2024-03-03T07:05:19.350623Z","shell.execute_reply.started":"2024-03-03T07:05:19.344765Z","shell.execute_reply":"2024-03-03T07:05:19.349807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#manually checking some inputs from the test data\ntest_data = training_data  = LoadCamvid(\"/kaggle/input/camvid/CamVid/class_dict.csv\" ,\"/kaggle/input/camvid/CamVid/test_labels\" ,  \"/kaggle/input/camvid/CamVid/test\" , transform = trans , target_transform = trans_target)\n\nimg , onehot , label = test_data[150]\n# img , onehot , label\nlabel = label.to(torch.int32)\nplt.imshow(img.permute(1,2,0))\nplt.show()\nplt.imshow(label.permute(1,2,0))\nplt.show()\noutput = fcn(img.unsqueeze(dim = 0))\npred_mask = rev_one_hot(mappings ,prob_to_one_hot(output[0]))\npred_mask = pred_mask.to(torch.int32)\nplt.imshow(pred_mask.permute(1,2,0).detach().to('cpu'))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T07:06:27.471119Z","iopub.execute_input":"2024-03-03T07:06:27.472096Z","iopub.status.idle":"2024-03-03T07:06:28.669823Z","shell.execute_reply.started":"2024-03-03T07:06:27.472056Z","shell.execute_reply":"2024-03-03T07:06:28.668855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img , onehot , label = test_data[100]\n# img , onehot , label\nlabel = label.to(torch.int32)\nplt.imshow(img.permute(1,2,0))\nplt.show()\nplt.imshow(label.permute(1,2,0))\nplt.show()\noutput = fcn(img.unsqueeze(dim = 0))\npred_mask = rev_one_hot(mappings ,prob_to_one_hot(output[0]))\npred_mask = pred_mask.to(torch.int32)\nplt.imshow(pred_mask.permute(1,2,0).detach().to('cpu'))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T07:05:20.780727Z","iopub.execute_input":"2024-03-03T07:05:20.781108Z","iopub.status.idle":"2024-03-03T07:05:21.867783Z","shell.execute_reply.started":"2024-03-03T07:05:20.781076Z","shell.execute_reply":"2024-03-03T07:05:21.866825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img , onehot , label = test_data[200]\n# img , onehot , label\nlabel = label.to(torch.int32)\nplt.imshow(img.permute(1,2,0))\nplt.show()\nplt.imshow(label.permute(1,2,0))\nplt.show()\noutput = fcn(img.unsqueeze(dim = 0))\npred_mask = rev_one_hot(mappings ,prob_to_one_hot(output[0]))\npred_mask = pred_mask.to(torch.int32)\nplt.imshow(pred_mask.permute(1,2,0).detach().to('cpu'))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T07:05:21.869054Z","iopub.execute_input":"2024-03-03T07:05:21.869368Z","iopub.status.idle":"2024-03-03T07:05:23.003423Z","shell.execute_reply.started":"2024-03-03T07:05:21.869343Z","shell.execute_reply":"2024-03-03T07:05:23.002431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img , onehot , label = test_data[25]\n# img , onehot , label\nlabel = label.to(torch.int32)\nplt.imshow(img.permute(1,2,0))\nplt.show()\nplt.imshow(label.permute(1,2,0))\nplt.show()\noutput = fcn(img.unsqueeze(dim = 0))\npred_mask = rev_one_hot(mappings ,prob_to_one_hot(output[0]))\npred_mask = pred_mask.to(torch.int32)\nplt.imshow(pred_mask.permute(1,2,0).detach().to('cpu'))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T07:05:23.004741Z","iopub.execute_input":"2024-03-03T07:05:23.005042Z","iopub.status.idle":"2024-03-03T07:05:24.150897Z","shell.execute_reply.started":"2024-03-03T07:05:23.005016Z","shell.execute_reply":"2024-03-03T07:05:24.149922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img , onehot , label = test_data[125]\n# img , onehot , label\nlabel = label.to(torch.int32)\nplt.imshow(img.permute(1,2,0))\nplt.show()\nplt.imshow(label.permute(1,2,0))\nplt.show()\noutput = fcn(img.unsqueeze(dim = 0))\npred_mask = rev_one_hot(mappings ,prob_to_one_hot(output[0]))\npred_mask = pred_mask.to(torch.int32)\nplt.imshow(pred_mask.permute(1,2,0).detach().to('cpu'))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T07:05:24.155879Z","iopub.execute_input":"2024-03-03T07:05:24.156220Z","iopub.status.idle":"2024-03-03T07:05:25.223901Z","shell.execute_reply.started":"2024-03-03T07:05:24.156190Z","shell.execute_reply":"2024-03-03T07:05:25.222918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img , onehot , label = test_data[225]\n# img , onehot , label\nlabel = label.to(torch.int32)\nplt.imshow(img.permute(1,2,0))\nplt.show()\nplt.imshow(label.permute(1,2,0))\nplt.show()\noutput = fcn(img.unsqueeze(dim = 0))\npred_mask = rev_one_hot(mappings ,prob_to_one_hot(output[0]))\npred_mask = pred_mask.to(torch.int32)\nplt.imshow(pred_mask.permute(1,2,0).detach().to('cpu'))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T07:05:25.225092Z","iopub.execute_input":"2024-03-03T07:05:25.225416Z","iopub.status.idle":"2024-03-03T07:05:26.350723Z","shell.execute_reply.started":"2024-03-03T07:05:25.225388Z","shell.execute_reply":"2024-03-03T07:05:26.349617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img , onehot , label = test_data[80]\n# img , onehot , label\nlabel = label.to(torch.int32)\nplt.imshow(img.permute(1,2,0))\nplt.show()\nplt.imshow(label.permute(1,2,0))\nplt.show()\noutput = fcn(img.unsqueeze(dim = 0))\npred_mask = rev_one_hot(mappings ,prob_to_one_hot(output[0]))\npred_mask = pred_mask.to(torch.int32)\nplt.imshow(pred_mask.permute(1,2,0).detach().to('cpu'))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T07:05:26.351845Z","iopub.execute_input":"2024-03-03T07:05:26.352131Z","iopub.status.idle":"2024-03-03T07:05:27.475614Z","shell.execute_reply.started":"2024-03-03T07:05:26.352107Z","shell.execute_reply":"2024-03-03T07:05:27.474617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img , onehot , label = test_data[66]\n# img , onehot , label\nlabel = label.to(torch.int32)\nplt.imshow(img.permute(1,2,0))\nplt.show()\nplt.imshow(label.permute(1,2,0))\nplt.show()\noutput = fcn(img.unsqueeze(dim = 0))\npred_mask = rev_one_hot(mappings ,prob_to_one_hot(output[0]))\npred_mask = pred_mask.to(torch.int32)\nplt.imshow(pred_mask.permute(1,2,0).detach().to('cpu'))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T07:05:27.477122Z","iopub.execute_input":"2024-03-03T07:05:27.477709Z","iopub.status.idle":"2024-03-03T07:05:28.651976Z","shell.execute_reply.started":"2024-03-03T07:05:27.477672Z","shell.execute_reply":"2024-03-03T07:05:28.651035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}